{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec5ce81",
   "metadata": {},
   "source": [
    "### LLM-based peer review simulation setup\n",
    "\n",
    "This notebook configures an **LLM-based reviewer + editor pipeline** for ICLR-style peer review.\n",
    "\n",
    "We use the parsed manuscripts in `ICLR2025_papers/` and the historical **human reviews and decisions** in the CSV file to:\n",
    "\n",
    "- Construct reviewer prompts for an LLM to read each manuscript and produce a JSON review (summary, strengths, weaknesses, questions, decision).\n",
    "- Construct editor/area chair prompts for an LLM to read both **human reviews + LLM reviews** and output a final decision and rationale.\n",
    "\n",
    "The goal is to simulate how replacing some proportion of human reviewers with LLM agents changes **paper-level decisions and written reviews**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e1966e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/cathychen/Library/CloudStorage/OneDrive-Stanford/Research/MIMIR/ICLR_simulation/GABM_peer_review/ICLR2025_human_reviews_with_decision.csv\n",
      "Raw reviews shape: (35351, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_forum</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>decision</th>\n",
       "      <th>review_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>strengths</th>\n",
       "      <th>weaknesses</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zkNCWtw2fd</td>\n",
       "      <td>zkNCWtw2fd</td>\n",
       "      <td>Synergistic Approach for Simultaneous Optimiza...</td>\n",
       "      <td>Reject</td>\n",
       "      <td>UXJcl5skaD</td>\n",
       "      <td>ICLR.cc/2025/Conference/Submission14290/Review...</td>\n",
       "      <td>This paper introduces a hybrid batch training ...</td>\n",
       "      <td>1. Addresses a relevant challenge in multiling...</td>\n",
       "      <td>1. The primary contribution merely combines tw...</td>\n",
       "      <td>None.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zkNCWtw2fd</td>\n",
       "      <td>zkNCWtw2fd</td>\n",
       "      <td>Synergistic Approach for Simultaneous Optimiza...</td>\n",
       "      <td>Reject</td>\n",
       "      <td>bXNaEp660n</td>\n",
       "      <td>ICLR.cc/2025/Conference/Submission14290/Review...</td>\n",
       "      <td>The paper studies information retrieval tasks ...</td>\n",
       "      <td>The paper shows that two standard batching str...</td>\n",
       "      <td>1. Limited evaluations are only QA datasets (e...</td>\n",
       "      <td>1. Related to weakness1, could this proposed m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  paper_forum    paper_id                                              title  \\\n",
       "0  zkNCWtw2fd  zkNCWtw2fd  Synergistic Approach for Simultaneous Optimiza...   \n",
       "1  zkNCWtw2fd  zkNCWtw2fd  Synergistic Approach for Simultaneous Optimiza...   \n",
       "\n",
       "  decision   review_id                                        reviewer_id  \\\n",
       "0   Reject  UXJcl5skaD  ICLR.cc/2025/Conference/Submission14290/Review...   \n",
       "1   Reject  bXNaEp660n  ICLR.cc/2025/Conference/Submission14290/Review...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  This paper introduces a hybrid batch training ...   \n",
       "1  The paper studies information retrieval tasks ...   \n",
       "\n",
       "                                           strengths  \\\n",
       "0  1. Addresses a relevant challenge in multiling...   \n",
       "1  The paper shows that two standard batching str...   \n",
       "\n",
       "                                          weaknesses  \\\n",
       "0  1. The primary contribution merely combines tw...   \n",
       "1  1. Limited evaluations are only QA datasets (e...   \n",
       "\n",
       "                                           questions  \n",
       "0                                              None.  \n",
       "1  1. Related to weakness1, could this proposed m...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "# Path to the CSV created in 01_get_human_review.ipynb\n",
    "# Ensure this file exists in your directory or adjust the path\n",
    "DATA_PATH = Path(\"ICLR2025_human_reviews_with_decision.csv\")\n",
    "if not DATA_PATH.exists():\n",
    "    # Fallback if the specific filename differs\n",
    "    DATA_PATH = Path(\"ICLR2025_human_reviews.csv\")\n",
    "\n",
    "print(f\"Loading data from: {DATA_PATH.resolve()}\")\n",
    "\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_PATH}. Please run 01_get_human_review.ipynb first.\")\n",
    "\n",
    "reviews_df = pd.read_csv(DATA_PATH)\n",
    "print(\"Raw reviews shape:\", reviews_df.shape)\n",
    "reviews_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c85a712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has OPENAI_API_KEY: True\n",
      "Key prefix: sk-proj ...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env in the current working directory\n",
    "load_dotenv()\n",
    "\n",
    "# Optional sanity check (does NOT print the whole key)\n",
    "key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(\"Has OPENAI_API_KEY:\", bool(key))\n",
    "if key:\n",
    "    print(\"Key prefix:\", key[:7], \"...\")  # Just to confirm it's being read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a419d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper-level stats shape: (8727, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>n_reviews</th>\n",
       "      <th>decision</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00SnKBGTsz</td>\n",
       "      <td>4</td>\n",
       "      <td>Accept (Spotlight)</td>\n",
       "      <td>DataEnvGym: Data Generation Agents in Teacher ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00ezkB2iZf</td>\n",
       "      <td>4</td>\n",
       "      <td>Reject</td>\n",
       "      <td>MedFuzz: Exploring the Robustness of Large Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01wMplF8TL</td>\n",
       "      <td>4</td>\n",
       "      <td>Reject</td>\n",
       "      <td>INSTRUCTION-FOLLOWING LLMS FOR TIME SERIES PRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029hDSVoXK</td>\n",
       "      <td>5</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>Dynamic Neural Fortresses: An Adaptive Shield ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02Od16GFRW</td>\n",
       "      <td>3</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Ensembles provably learn equivariance through ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id  n_reviews            decision  \\\n",
       "0  00SnKBGTsz          4  Accept (Spotlight)   \n",
       "1  00ezkB2iZf          4              Reject   \n",
       "2  01wMplF8TL          4              Reject   \n",
       "3  029hDSVoXK          5     Accept (Poster)   \n",
       "4  02Od16GFRW          3              Reject   \n",
       "\n",
       "                                               title  \n",
       "0  DataEnvGym: Data Generation Agents in Teacher ...  \n",
       "1  MedFuzz: Exploring the Robustness of Large Lan...  \n",
       "2  INSTRUCTION-FOLLOWING LLMS FOR TIME SERIES PRE...  \n",
       "3  Dynamic Neural Fortresses: An Adaptive Shield ...  \n",
       "4  Ensembles provably learn equivariance through ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse numeric rating from strings like \"8: Strong accept\" if needed (optional for this flow but good for reference)\n",
    "def parse_rating(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, (int, float)):\n",
    "        return float(val)\n",
    "    s = str(val).strip()\n",
    "    # Try splitting on ':' first, then space\n",
    "    for sep in (\":\", \" \"):\n",
    "        try:\n",
    "            num = float(s.split(sep)[0])\n",
    "            return num\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    return np.nan\n",
    "\n",
    "if \"rating\" in reviews_df.columns:\n",
    "    reviews_df[\"rating_num\"] = reviews_df[\"rating\"].apply(parse_rating)\n",
    "    print(\"Fraction of reviews with parsed numeric rating:\", reviews_df[\"rating_num\"].notna().mean())\n",
    "\n",
    "\n",
    "# Aggregate to paper-level statistics (useful for selecting papers to simulate)\n",
    "\n",
    "paper_stats = (\n",
    "    reviews_df.groupby(\"paper_id\")\n",
    "    .agg(\n",
    "        n_reviews=(\"review_id\", \"count\"),\n",
    "        decision=(\"decision\", lambda x: x.dropna().iloc[0] if len(x.dropna()) else np.nan),\n",
    "        title=(\"title\", \"first\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Paper-level stats shape:\", paper_stats.shape)\n",
    "paper_stats.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eef42a",
   "metadata": {},
   "source": [
    "### LLM-based reviewer/editor simulation (decision + written review)\n",
    "\n",
    "In this section we setup the **LLM-based reviewer/editor simulation**.\n",
    "\n",
    "For each manuscript we:\n",
    "\n",
    "1. **Load the parsed manuscript text** from `ICLR2025_papers/<paper_id>/<paper_id>.tei.xml`.\n",
    "2. **Prompt an LLM reviewer** to read the manuscript and output JSON with `summary`, `strengths`, and `weaknesses`. \n",
    "3. **Prompt an LLM editor/area chair** to read the *human reviews from CSV* plus the LLM-generated review(s) and make a final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "786465ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PAPERS_ROOT = Path(\"ICLR2025_papers\")\n",
    "\n",
    "\n",
    "def load_tei_for_paper(paper_id: str) -> Path:\n",
    "    \"\"\"Return the TEI XML path for a given ICLR paper_id.\n",
    "\n",
    "    Assumes the structure ICLR2025_papers/<paper_id>/<paper_id>.tei.xml.\n",
    "    \"\"\"\n",
    "    # Sometimes folder name is paper_id, sometimes it might be forum ID. \n",
    "    # Adjust if your folder structure differs. \n",
    "    tei_path = PAPERS_ROOT / paper_id / f\"{paper_id}.tei.xml\"\n",
    "    \n",
    "    # Fallback check: sometimes folder exists but file name might differ slightly or be in a subfolder\n",
    "    if not tei_path.exists():\n",
    "        # Try just the paper_id folder if the tei file is named differently\n",
    "        potential_files = list((PAPERS_ROOT / paper_id).glob(\"*.tei.xml\"))\n",
    "        if potential_files:\n",
    "            return potential_files[0]\n",
    "            \n",
    "    if not tei_path.exists():\n",
    "        raise FileNotFoundError(f\"TEI file not found for paper_id={paper_id}: {tei_path}\")\n",
    "    return tei_path\n",
    "\n",
    "def extract_text_from_tei(tei_path: Path, max_chars: int = 30000) -> str:\n",
    "    \"\"\"Lightweight TEI â†’ plain text extractor (title + abstract + body paragraphs).\n",
    "\n",
    "    This is intentionally simple; you can swap it out for a more faithful converter later.\n",
    "    \"\"\"\n",
    "\n",
    "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "    try:\n",
    "        tree = ET.parse(tei_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        return f\"[Error parsing TEI XML: {e}]\"\n",
    "\n",
    "    parts: list[str] = []\n",
    "\n",
    "    # Title\n",
    "    title_el = root.find(\".//tei:titleStmt/tei:title\", ns)\n",
    "    if title_el is not None and (title_el.text or \"\").strip():\n",
    "        parts.append(title_el.text.strip())\n",
    "\n",
    "    # Abstract\n",
    "    for p in root.findall(\".//tei:abstract//tei:p\", ns):\n",
    "        if p.text and p.text.strip():\n",
    "            parts.append(p.text.strip())\n",
    "\n",
    "    # Body\n",
    "    for p in root.findall(\".//tei:body//tei:p\", ns):\n",
    "        if p.text and p.text.strip():\n",
    "            parts.append(p.text.strip())\n",
    "\n",
    "    text = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    # Truncate if too long for context window\n",
    "    if max_chars is not None and len(text) > max_chars:\n",
    "        text = text[:max_chars] + \"\\n\\n[TRUNCATED FOR PROMPT LENGTH]\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_paper_text(paper_id: str, max_chars: int = 30000) -> str:\n",
    "    tei_path = load_tei_for_paper(paper_id)\n",
    "    return extract_text_from_tei(tei_path, max_chars=max_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30171d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle human reviews for a given paper into a simple JSON-friendly structure\n",
    "\n",
    "HUMAN_COLUMNS = [\"review_id\", \"reviewer_id\", \"summary\", \"strengths\", \"weaknesses\", \"questions\", \"decision\"]\n",
    "# Ensure columns exist\n",
    "valid_cols = [c for c in HUMAN_COLUMNS if c in reviews_df.columns]\n",
    "\n",
    "def get_human_reviews_for_paper(paper_id: str) -> list[dict]:\n",
    "    rows = reviews_df.loc[reviews_df[\"paper_id\"] == paper_id, valid_cols]\n",
    "    reviews: list[dict] = []\n",
    "    for _, r in rows.iterrows():\n",
    "        review_dict = {\n",
    "            \"source\": \"human\",\n",
    "            \"review_id\": r.get(\"review_id\"),\n",
    "            \"reviewer_id\": r.get(\"reviewer_id\"),\n",
    "            \"summary\": r.get(\"summary\"),\n",
    "            \"strengths\": r.get(\"strengths\"),\n",
    "            \"weaknesses\": r.get(\"weaknesses\"),\n",
    "            \"questions\": r.get(\"questions\"),\n",
    "            \"rating\": r.get(\"rating\"),\n",
    "            \"decision\": r.get(\"decision\"),\n",
    "        }\n",
    "        reviews.append(review_dict)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40365f6",
   "metadata": {},
   "source": [
    "### AutoGen Simulation Helpers\n",
    "Copied and adapted from LLM_review_simulation.ipynb for use with our parsed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b252324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "try:\n",
    "    from autogen import AssistantAgent, UserProxyAgent\n",
    "except ImportError:\n",
    "    print(\"AutoGen not installed. Please run: pip install autogen-agentchat\")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "DEFAULT_LLM_CONFIG = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 1200,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "DECISION_LABELS = [\"oral\", \"spotlight\", \"poster\", \"reject\"]\n",
    "# Adjust decisions if your dataset uses different labels (e.g. \"Accept (Oral)\", \"Reject\")\n",
    "# For ICLR, these are standard.\n",
    "\n",
    "SENIORITY_LABELS = {\n",
    "    \"grad\": \"Graduate student reviewer\",\n",
    "    \"junior\": \"Junior faculty reviewer\",\n",
    "    \"senior\": \"Senior faculty reviewer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf10ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWER_PROMPT_TEMPLATE = \"\"\"You are serving as an ICLR reviewer with expertise in {expertise}.\n",
    "Seniority: {seniority_label}.\n",
    "\n",
    "Guidelines:\n",
    "- Carefully read the entire manuscript text provided to you.\n",
    "- Wait until you receive the message END_OF_PAPER before replying.\n",
    "- Provide a fair, evidence-based review using first person plural.\n",
    "- Produce exactly one JSON object as specified below; do not add commentary or prose outside JSON.\n",
    "\n",
    "JSON schema (keys are required):\n",
    "{{\n",
    "  \"persona\": {{\"expertise\": string, \"seniority\": string}},\n",
    "  \"review\": {{\n",
    "    \"summary\": string,\n",
    "    \"strengths\": [string],\n",
    "    \"weaknesses\": [string],\n",
    "    \"questions\": [string],\n",
    "    \"confidence\": integer 1-5,\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55e815d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updated Editor Prompt to handle both Human and LLM reviews\n",
    "# EDITOR_PROMPT = \"\"\"You are the area chair for an ICLR submission.\n",
    "# You will receive the manuscript text plus a list of reviews (some human, some LLM-generated).\n",
    "# Tasks:\n",
    "# 1. Identify consensus and disagreements across ALL reviews.\n",
    "# 2. Justify a single decision in {decisions}.\n",
    "# 3. Return a strict JSON object without code fences: \n",
    "# {{\n",
    "#   \"decision\": one of {decisions},\n",
    "#   \"confidence\": float between 0 and 1,\n",
    "#   \"rationale\": short paragraph,\n",
    "#   \"highlights\": [string]\n",
    "# }}\n",
    "# \"\"\".format(decisions=DECISION_LABELS)\n",
    "\n",
    "EDITOR_PROMPT = \"\"\"You are serving as the Area Chair for an ICLR submission.\n",
    "You will receive the manuscript text plus a list of reviews (some human-written, some LLM-generated).\n",
    "\n",
    "Your responsibilities:\n",
    "\n",
    "1. Analyze all reviews:\n",
    "   - Identify clear points of consensus across reviewers.\n",
    "   - Identify and explain disagreements, including which reviewers disagree and why.\n",
    "   - Flag any reviewer misunderstandings, unsupported claims, or inconsistencies.\n",
    "\n",
    "2. Conduct an independent Area Chair assessment:\n",
    "   - Evaluate novelty, technical correctness, significance, empirical rigor, and clarity.\n",
    "   - Weigh reviewer feedback appropriately, but do not rely on it blindly.\n",
    "   - Distinguish between major and minor concerns.\n",
    "   - Note any methodological flaws, missing experiments, ethical issues, or correctness problems that materially affect acceptance.\n",
    "\n",
    "3. Make and justify the final decision (one of {decisions}):\n",
    "   - Base the decision on reviewer consensus, resolved disagreements, and your own AC-level judgment.\n",
    "   - Ensure the justification reflects ICLR standards: correctness > novelty > significance > clarity.\n",
    "   - The justification must be factual, grounded in the manuscript + reviews, and internally consistent.\n",
    "\n",
    "4. Return a strict JSON object with no code fences:\n",
    "{{\n",
    "  \"decision\": one of {decisions},\n",
    "  \"confidence\": float between 0 and 1,\n",
    "  \"rationale\": \"A concise paragraph explaining the decision, referencing consensus, disagreements, and your independent assessment.\",\n",
    "  \"highlights\": [\n",
    "    \"Key factor influencing the decision\",\n",
    "    \"Another key factor\",\n",
    "    \"Major consensus or disagreement point\",\n",
    "    \"Any important caveat or uncertainty\"\n",
    "  ]\n",
    "}}\n",
    "\"\"\".format(decisions=DECISION_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9be51ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReviewerPersona:\n",
    "    name: str\n",
    "    expertise: str\n",
    "    seniority: str\n",
    "\n",
    "    @property\n",
    "    def seniority_label(self) -> str:\n",
    "        return SENIORITY_LABELS.get(self.seniority, self.seniority)\n",
    "\n",
    "def require_api_key() -> str:\n",
    "    # Ensure you have set this environment variable!\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"WARNING: OPENAI_API_KEY not set. Simulation calls will fail.\")\n",
    "        return \"\"\n",
    "    return api_key\n",
    "\n",
    "def build_llm_config(overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    config: Dict[str, Any] = {**DEFAULT_LLM_CONFIG}\n",
    "    if overrides:\n",
    "        config.update(overrides)\n",
    "    \n",
    "    # Filter config list to only include valid keys if using OpenAI\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": config.get(\"model\", MODEL_NAME),\n",
    "            \"api_key\": require_api_key(),\n",
    "        }\n",
    "    ]\n",
    "    config[\"config_list\"] = config_list\n",
    "    return config\n",
    "\n",
    "def make_reviewer_agent(persona: ReviewerPersona, overrides: Optional[Dict[str, Any]] = None) -> AssistantAgent:\n",
    "    llm_config = build_llm_config(overrides)\n",
    "    system_message = REVIEWER_PROMPT_TEMPLATE.format(\n",
    "        expertise=persona.expertise,\n",
    "        seniority_label=persona.seniority_label,\n",
    "    )\n",
    "    return AssistantAgent(\n",
    "        name=persona.name,\n",
    "        system_message=system_message,\n",
    "        llm_config=llm_config,\n",
    "    )\n",
    "def make_editor_agent(overrides: Optional[Dict[str, Any]] = None) -> AssistantAgent:\n",
    "    llm_config = build_llm_config(overrides)\n",
    "    return AssistantAgent(\n",
    "        name=\"area_chair\",\n",
    "        system_message=EDITOR_PROMPT,\n",
    "        llm_config=llm_config,\n",
    "    )\n",
    "\n",
    "def strip_code_fences(text: str) -> str:\n",
    "    stripped = text.strip()\n",
    "    if stripped.startswith(\"```\") and stripped.endswith(\"```\"):\n",
    "        stripped = stripped.strip(\"`\")\n",
    "        if \"\\n\" in stripped:\n",
    "            _, _, remainder = stripped.partition(\"\\n\")\n",
    "            stripped = remainder\n",
    "    return stripped.strip()\n",
    "\n",
    "def parse_json_response(raw_text: str) -> Dict[str, Any]:\n",
    "    cleaned = strip_code_fences(raw_text)\n",
    "    cleaned = cleaned.replace(\"\\u200b\", \"\").strip()\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"Empty response\")\n",
    "    return json.loads(cleaned)\n",
    "\n",
    "def message_content(message: Any) -> str:\n",
    "    if message is None: return \"\"\n",
    "    if isinstance(message, str): return message\n",
    "    if isinstance(message, dict): return str(message.get(\"content\", \"\"))\n",
    "    if hasattr(message, \"content\"): return str(message.content)\n",
    "    return str(message)\n",
    "\n",
    "def last_reply(agent: AssistantAgent, conversation_partner: UserProxyAgent) -> str:\n",
    "    history = getattr(agent, \"_oai_messages\", {})\n",
    "    if conversation_partner in history:\n",
    "        msg = agent.last_message(conversation_partner)\n",
    "    else:\n",
    "        msg = agent.last_message()\n",
    "    return message_content(msg)\n",
    "\n",
    "def send_with_retry(sender, recipient, message, request_reply, silent=True, max_attempts=3):\n",
    "    last_error = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            return sender.send(recipient=recipient, message=message, request_reply=request_reply, silent=silent)\n",
    "        except Exception as exc:\n",
    "            last_error = exc\n",
    "            time.sleep(min(4, attempt) * 2)\n",
    "    if last_error: raise last_error\n",
    "\n",
    "def collect_reviewer_output(persona: ReviewerPersona, paper_text: str, llm_overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    reviewer = make_reviewer_agent(persona, overrides=llm_overrides)\n",
    "    orchestrator = UserProxyAgent(name=f\"proxy_for_{persona.name}\", human_input_mode=\"NEVER\", code_execution_config=False)\n",
    "\n",
    "    intro = \"You will receive the full manuscript. Read it completely and respond only after prompted.\"\n",
    "    send_with_retry(orchestrator, reviewer, intro, request_reply=False)\n",
    "    send_with_retry(orchestrator, reviewer, f\"PAPER_TEXT\\n\\n{paper_text}\", request_reply=False)\n",
    "\n",
    "    final_prompt = \"END_OF_PAPER. Produce the JSON object now. Output valid JSON only without fences.\"\n",
    "    \n",
    "    for _ in range(3):\n",
    "        send_with_retry(orchestrator, reviewer, final_prompt, request_reply=True, silent=False)\n",
    "        try:\n",
    "            return parse_json_response(last_reply(reviewer, orchestrator))\n",
    "        except Exception:\n",
    "            send_with_retry(orchestrator, reviewer, \"Invalid JSON. Respond again with JSON only.\", request_reply=False)\n",
    "    raise RuntimeError(\"Unable to collect reviewer output\")\n",
    "\n",
    "def collect_editor_decision(paper_text: str, reviews: List[Dict[str, Any]], llm_overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    editor = make_editor_agent(overrides=llm_overrides)\n",
    "    orchestrator = UserProxyAgent(name=\"proxy_for_editor\", human_input_mode=\"NEVER\", code_execution_config=False)\n",
    "\n",
    "    send_with_retry(orchestrator, editor, \"You will receive the paper text followed by reviews.\", request_reply=False)\n",
    "    send_with_retry(orchestrator, editor, f\"PAPER_TEXT_START\\n{paper_text}\\nPAPER_TEXT_END\", request_reply=False)\n",
    "    \n",
    "    # Convert reviews to string payload\n",
    "    reviews_payload = json.dumps(reviews, indent=2)\n",
    "    send_with_retry(orchestrator, editor, f\"REVIEWS_START\\n{reviews_payload}\\nREVIEWS_END\", request_reply=False)\n",
    "\n",
    "    final_prompt = \"Using the rubric, output the JSON decision now.\"\n",
    "    \n",
    "    for _ in range(3):\n",
    "        send_with_retry(orchestrator, editor, final_prompt, request_reply=True, silent=False)\n",
    "        try:\n",
    "            return parse_json_response(last_reply(editor, orchestrator))\n",
    "        except Exception:\n",
    "            send_with_retry(orchestrator, editor, \"Invalid JSON. Respond again with JSON only.\", request_reply=False)\n",
    "    raise RuntimeError(\"Unable to collect editor decision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a51c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1876521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Wiring it all together ===\n",
    "\n",
    "def call_llm_reviewer(paper_text: str, *, metadata: dict | None = None) -> dict:\n",
    "    \"\"\"Concrete implementation using AutoGen.\"\"\"\n",
    "    # Create a random persona for variety or use metadata to set specific expertise\n",
    "    persona = ReviewerPersona(\n",
    "        name=\"sim_reviewer\",\n",
    "        expertise=\"machine learning\",  # could be inferred from paper title/content in future\n",
    "        seniority=random.choice([\"grad\", \"junior\", \"senior\"])\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = collect_reviewer_output(persona, paper_text)\n",
    "        # Normalize keys if needed\n",
    "        result[\"source\"] = \"llm\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Reviewer failed: {e}\")\n",
    "        return {\"source\": \"llm\", \"error\": str(e)}\n",
    "\n",
    "def call_llm_editor(paper_text: str, *, human_reviews: list[dict], llm_reviews: list[dict], metadata: dict | None = None) -> dict:\n",
    "    \"\"\"Concrete implementation using AutoGen.\"\"\"\n",
    "    all_reviews = human_reviews + llm_reviews\n",
    "    try:\n",
    "        return collect_editor_decision(paper_text, all_reviews)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Editor failed: {e}\")\n",
    "        return {\"decision\": \"reject\", \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8cb5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level LLM simulation for a single paper\n",
    "\n",
    "def run_llm_review_simulation_for_paper(\n",
    "    paper_id: str,\n",
    "    *,\n",
    "    n_llm_reviewers: int = 1,\n",
    "    replace_human: bool = False,\n",
    "    max_chars: int = 30000,\n",
    ") -> dict:\n",
    "    \"\"\"Run the 3-step LLM pipeline for one paper_id.\n",
    "\n",
    "    Args:\n",
    "        paper_id: The paper ID to simulate.\n",
    "        n_llm_reviewers: How many LLM reviewers to include.\n",
    "        replace_human: If True, effectively 'switches' n_llm_reviewers human reviews with LLM reviews.\n",
    "                       (If there are fewer human reviews than n_llm_reviewers, it replaces all of them).\n",
    "                       If False, adds n_llm_reviewers to the existing human panel.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        paper_text = get_paper_text(paper_id, max_chars=max_chars)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Skipping {paper_id}: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    human_reviews = get_human_reviews_for_paper(paper_id)\n",
    "\n",
    "    # Implement 'switch' logic: if replace_human is True, remove k random human reviews\n",
    "    if replace_human and len(human_reviews) > 0:\n",
    "        # Remove up to n_llm_reviewers human reviews to make space for the agents\n",
    "        n_to_remove = min(n_llm_reviewers, len(human_reviews))\n",
    "        # Simple slice (could be random.sample if order mattered more)\n",
    "        human_reviews_to_keep = human_reviews[n_to_remove:]\n",
    "        final_human_reviews = human_reviews_to_keep\n",
    "    else:\n",
    "        final_human_reviews = human_reviews\n",
    "\n",
    "    llm_reviews: list[dict] = []\n",
    "    for i in range(n_llm_reviewers):\n",
    "        print(f\"Generating LLM review {i+1}/{n_llm_reviewers}...\")\n",
    "        llm_review = call_llm_reviewer(paper_text, metadata={\"paper_id\": paper_id})\n",
    "        llm_reviews.append(llm_review)\n",
    "\n",
    "    print(\"Generating Editor decision...\")\n",
    "    editor_decision = call_llm_editor(\n",
    "        paper_text,\n",
    "        human_reviews=final_human_reviews,\n",
    "        llm_reviews=llm_reviews,\n",
    "        metadata={\"paper_id\": paper_id},\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"paper_id\": paper_id,\n",
    "        \"human_reviews_used\": final_human_reviews,\n",
    "        \"llm_reviews\": llm_reviews,\n",
    "        \"editor_decision\": editor_decision,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea96841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_llm_composition_for_paper(\n",
    "    paper_id: str,\n",
    "    llm_counts: list[int],\n",
    "    n_rounds: int = 3,\n",
    "    replace_human: bool = False,\n",
    "    max_chars: int = 30000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"For one paper, run the LLM reviewer+editor pipeline with different numbers of LLM reviews.\n",
    "    \n",
    "    Args:\n",
    "        llm_counts: list of integers, e.g. [0, 1, 2].\n",
    "        replace_human: If True, we SWAP human reviewers for LLM reviewers.\n",
    "                       If False, we ADD LLM reviewers to the panel.\n",
    "    \"\"\"\n",
    "\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for n_llm in llm_counts:\n",
    "        for round_idx in range(1, n_rounds + 1):\n",
    "            print(f\"--- Simulating: {n_llm} LLM reviewers (Round {round_idx}/{n_rounds}) ---\")\n",
    "            sim = run_llm_review_simulation_for_paper(\n",
    "                paper_id=paper_id,\n",
    "                n_llm_reviewers=n_llm,\n",
    "                replace_human=replace_human,\n",
    "                max_chars=max_chars,\n",
    "            )\n",
    "            \n",
    "            if \"error\" in sim:\n",
    "                print(f\"Skipping due to error: {sim['error']}\")\n",
    "                continue\n",
    "\n",
    "            ed = sim.get(\"editor_decision\", {}) or {}\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"round\": round_idx,\n",
    "                    \"n_llm_reviewers\": n_llm,\n",
    "                    \"replace_human\": replace_human,\n",
    "                    \"editor_decision\": ed.get(\"decision\"),\n",
    "                    \"editor_rationale\": ed.get(\"rationale\"),\n",
    "                    \"editor_highlights\": ed.get(\"highlights\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e9bb",
   "metadata": {},
   "source": [
    "# Validate Editor Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b947437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct single-call LLM editor (one API call per paper)\n",
    "\n",
    "_client = None\n",
    "\n",
    "\n",
    "def get_openai_client() -> OpenAI:\n",
    "    \"\"\"Lazily construct a global OpenAI client using the same API key helper.\"\"\"\n",
    "    global _client\n",
    "    if _client is None:\n",
    "        api_key = require_api_key()\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"OPENAI_API_KEY not set; cannot call LLM editor.\")\n",
    "        _client = OpenAI(api_key=api_key)\n",
    "    return _client\n",
    "\n",
    "\n",
    "def call_llm_editor_single_call(\n",
    "    paper_text: str,\n",
    "    *,\n",
    "    reviews: list[dict],\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Call the editor model exactly once, with paper text and ALL reviews grouped together.\n",
    "\n",
    "    This bypasses AutoGen's multi-message protocol and retry loop, making\n",
    "    exactly one chat.completions.create call per paper.\n",
    "    \"\"\"\n",
    "    cfg: Dict[str, Any] = {**DEFAULT_LLM_CONFIG}\n",
    "    if overrides:\n",
    "        cfg.update(overrides)\n",
    "\n",
    "    model = cfg.get(\"model\", MODEL_NAME)\n",
    "    temperature = cfg.get(\"temperature\", 0.7)\n",
    "    max_tokens = cfg.get(\"max_tokens\", 1200)\n",
    "\n",
    "    reviews_payload = json.dumps(reviews, indent=2)\n",
    "\n",
    "    user_content = (\n",
    "        \"You will receive the full paper text and then the list of reviews.\\n\\n\"\n",
    "        \"PAPER_TEXT_START\\n\" + paper_text + \"\\nPAPER_TEXT_END\\n\\n\"\n",
    "        \"REVIEWS_START\\n\" + reviews_payload + \"\\nREVIEWS_END\\n\\n\"\n",
    "        \"Using the rubric, output the JSON decision now.\"\n",
    "    )\n",
    "\n",
    "    client = get_openai_client()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EDITOR_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    raw_text = resp.choices[0].message.content or \"\"\n",
    "    return parse_json_response(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77ab2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Validation: Check Editor alignment with Human Decisions ===\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def human_majority_editor_decision(human_reviews: list[dict]) -> dict:\n",
    "    \"\"\"Offline \"editor\" that uses only human review fields (no LLM).\n",
    "\n",
    "    Strategy:\n",
    "    - If per-review `decision` is available, take the majority label.\n",
    "    - Otherwise, fall back to mean numeric rating (if present) and map to a coarse label.\n",
    "    \"\"\"\n",
    "    if not human_reviews:\n",
    "        return {\n",
    "            \"decision\": \"reject\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"rationale\": \"No human reviews available; defaulting to reject.\",\n",
    "            \"highlights\": [],\n",
    "        }\n",
    "\n",
    "    # Collect decision strings (if present)\n",
    "    decisions = [\n",
    "        str(r.get(\"decision\")).strip()\n",
    "        for r in human_reviews\n",
    "        if r.get(\"decision\") not in (None, \"\", float(\"nan\"))\n",
    "    ]\n",
    "\n",
    "    if decisions:\n",
    "        counts = Counter(decisions)\n",
    "        top_decision, top_count = counts.most_common(1)[0]\n",
    "        confidence = top_count / len(decisions)\n",
    "        return {\n",
    "            \"decision\": top_decision,\n",
    "            \"confidence\": float(confidence),\n",
    "            \"rationale\": f\"Chosen by majority of human reviews ({top_count}/{len(decisions)}).\",\n",
    "            \"highlights\": [],\n",
    "        }\n",
    "\n",
    "    # Fallback: use mean numeric rating, if available\n",
    "    ratings = []\n",
    "    for r in human_reviews:\n",
    "        if \"rating_num\" in r and r[\"rating_num\"] is not None:\n",
    "            ratings.append(float(r[\"rating_num\"]))\n",
    "        elif \"rating\" in r and r[\"rating\"] is not None:\n",
    "            ratings.append(parse_rating(r[\"rating\"]))\n",
    "\n",
    "    ratings = [x for x in ratings if x == x]  # drop NaNs\n",
    "    if ratings:\n",
    "        avg = float(sum(ratings) / len(ratings))\n",
    "        if avg >= 8:\n",
    "            label = \"oral\"\n",
    "        elif avg >= 7:\n",
    "            label = \"spotlight\"\n",
    "        elif avg >= 6:\n",
    "            label = \"poster\"\n",
    "        else:\n",
    "            label = \"reject\"\n",
    "        return {\n",
    "            \"decision\": label,\n",
    "            \"confidence\": 0.5,\n",
    "            \"rationale\": f\"Heuristic decision from mean rating {avg:.2f}.\",\n",
    "            \"highlights\": [],\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"decision\": \"reject\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"rationale\": \"Insufficient structured signal in human reviews; defaulting to reject.\",\n",
    "        \"highlights\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_editor_on_human_reviews(\n",
    "    n_papers: int = 5,\n",
    "    seed: int = 42,\n",
    "    max_chars: int = 30000,\n",
    "    use_llm: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Validate either the LLM editor or an offline human-only rule.\n",
    "\n",
    "    Args:\n",
    "        n_papers: Number of papers to sample for validation.\n",
    "        seed: Random seed for sampling.\n",
    "        max_chars: Text limit for manuscript.\n",
    "        use_llm: If True, call the LLM editor; if False, use `human_majority_editor_decision`.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: [paper_id, actual_decision, llm_decision, match, rationale]\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for papers that actually have a decision and reviews\n",
    "    valid_papers = paper_stats[paper_stats[\"decision\"].notna() & (paper_stats[\"n_reviews\"] > 0)]\n",
    "\n",
    "    if len(valid_papers) == 0:\n",
    "        print(\"No papers found with both reviews and decisions to validate against.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sample = valid_papers.sample(n=min(n_papers, len(valid_papers)), random_state=seed)\n",
    "\n",
    "    results = []\n",
    "    mode = \"LLM\" if use_llm else \"human-majority heuristic\"\n",
    "    print(f\"Validating Editor ({mode}) on {len(sample)} papers...\")\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        pid = row[\"paper_id\"]\n",
    "        actual_decision = row[\"decision\"]\n",
    "\n",
    "        try:\n",
    "            paper_text = get_paper_text(pid, max_chars=max_chars)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Skipping {pid} (PDF text not found)\")\n",
    "            continue\n",
    "\n",
    "        human_reviews = get_human_reviews_for_paper(pid)\n",
    "\n",
    "        # Either call LLM editor (single API call) or offline heuristic\n",
    "        print(f\"  Processing {pid} (Actual: {actual_decision})...\")\n",
    "        if use_llm:\n",
    "            all_reviews = human_reviews  # we only use human reviews in this validation\n",
    "            llm_out = call_llm_editor_single_call(\n",
    "                paper_text=paper_text,\n",
    "                reviews=all_reviews,\n",
    "            )\n",
    "        else:\n",
    "            llm_out = human_majority_editor_decision(human_reviews)\n",
    "\n",
    "        llm_decision = llm_out.get(\"decision\", \"error\")\n",
    "        rationale = llm_out.get(\"rationale\", \"\")\n",
    "\n",
    "        # Simple fuzzy match for decision labels (e.g. \"accept (oral)\" vs \"oral\")\n",
    "        # Normalize both to lower case\n",
    "        act_norm = str(actual_decision).lower()\n",
    "        llm_norm = str(llm_decision).lower()\n",
    "\n",
    "        # Define a match if the main category agrees (Accept vs Reject)\n",
    "        # or if the specific label matches.\n",
    "        is_match = False\n",
    "        if \"reject\" in act_norm and \"reject\" in llm_norm:\n",
    "            is_match = True\n",
    "        elif \"accept\" in act_norm and \"accept\" in llm_norm:\n",
    "            is_match = True\n",
    "        elif \"oral\" in act_norm and \"oral\" in llm_norm:\n",
    "            is_match = True\n",
    "        elif \"spotlight\" in act_norm and \"spotlight\" in llm_norm:\n",
    "            is_match = True\n",
    "        elif \"poster\" in act_norm and \"poster\" in llm_norm:\n",
    "            is_match = True\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"paper_id\": pid,\n",
    "                \"actual_decision\": actual_decision,\n",
    "                \"llm_decision\": llm_decision,\n",
    "                \"match\": is_match,\n",
    "                \"llm_rationale\": rationale,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "            # Helper to map decision string to binary (Accept=1, Reject=0)\n",
    "            def to_binary(d_str):\n",
    "                s = str(d_str).lower()\n",
    "                if \"reject\" in s:\n",
    "                    return 0\n",
    "                # Assume \"accept\", \"oral\", \"spotlight\", \"poster\" are all accept\n",
    "                if any(k in s for k in [\"accept\", \"oral\", \"spotlight\", \"poster\"]):\n",
    "                    return 1\n",
    "                return 0\n",
    "\n",
    "            y_true = df[\"actual_decision\"].apply(to_binary)\n",
    "            y_pred = df[\"llm_decision\"].apply(to_binary)\n",
    "\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "            print(\"\\nValidation Complete.\")\n",
    "            print(f\"Accuracy: {acc:.2%}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"\\nValidation Complete. (sklearn not found, skipping F1/Accuracy)\")\n",
    "            print(f\"Raw Agreement Rate: {df['match'].mean():.2%}\")\n",
    "\n",
    "    if not df.empty:\n",
    "        accuracy = df[\"match\"].mean()\n",
    "        print(f\"\\nValidation Complete. Agreement Rate: {accuracy:.2%}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3db85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Editor (LLM) on 10 papers...\n",
      "  Processing 7WUdjDhF38 (Actual: Reject)...\n",
      "  Processing xkgfLXZ4e0 (Actual: Accept (Poster))...\n",
      "  Processing SBCMNc3Mq3 (Actual: Accept (Oral))...\n",
      "  Processing OBjF5I4PWg (Actual: Accept (Poster))...\n",
      "  Processing aKcd7ImG5e (Actual: Accept (Poster))...\n",
      "  Processing lBlHIQ1psv (Actual: Reject)...\n",
      "  Processing CPhqrV5Ehg (Actual: Reject)...\n",
      "  Processing 0Zot73kfLB (Actual: Reject)...\n",
      "  Processing Bl3e8HV9xW (Actual: Accept (Poster))...\n",
      "  Processing K2Tqn8R9pu (Actual: Accept (Poster))...\n",
      "\n",
      "Validation Complete. (sklearn not found, skipping F1/Accuracy)\n",
      "Raw Agreement Rate: 100.00%\n",
      "\n",
      "Validation Complete. Agreement Rate: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>actual_decision</th>\n",
       "      <th>llm_decision</th>\n",
       "      <th>match</th>\n",
       "      <th>llm_rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7WUdjDhF38</td>\n",
       "      <td>Reject</td>\n",
       "      <td>reject</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper presents an interesting approach to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xkgfLXZ4e0</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>poster</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper presents a novel investigation into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SBCMNc3Mq3</td>\n",
       "      <td>Accept (Oral)</td>\n",
       "      <td>oral</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper presents a significant contribution ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OBjF5I4PWg</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>poster</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper addresses a novel and relevant probl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aKcd7ImG5e</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>poster</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper presents a novel approach to time se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lBlHIQ1psv</td>\n",
       "      <td>Reject</td>\n",
       "      <td>reject</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper presents the ADOPD-Instruct dataset,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CPhqrV5Ehg</td>\n",
       "      <td>Reject</td>\n",
       "      <td>reject</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper proposes a low-rank autoregressive r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0Zot73kfLB</td>\n",
       "      <td>Reject</td>\n",
       "      <td>reject</td>\n",
       "      <td>True</td>\n",
       "      <td>The manuscript presents a novel approach, GVFi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bl3e8HV9xW</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>poster</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper introduces a novel solution concept,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>K2Tqn8R9pu</td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>poster</td>\n",
       "      <td>True</td>\n",
       "      <td>The manuscript presents a novel approach to Ga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id  actual_decision llm_decision  match  \\\n",
       "0  7WUdjDhF38           Reject       reject   True   \n",
       "1  xkgfLXZ4e0  Accept (Poster)       poster   True   \n",
       "2  SBCMNc3Mq3    Accept (Oral)         oral   True   \n",
       "3  OBjF5I4PWg  Accept (Poster)       poster   True   \n",
       "4  aKcd7ImG5e  Accept (Poster)       poster   True   \n",
       "5  lBlHIQ1psv           Reject       reject   True   \n",
       "6  CPhqrV5Ehg           Reject       reject   True   \n",
       "7  0Zot73kfLB           Reject       reject   True   \n",
       "8  Bl3e8HV9xW  Accept (Poster)       poster   True   \n",
       "9  K2Tqn8R9pu  Accept (Poster)       poster   True   \n",
       "\n",
       "                                       llm_rationale  \n",
       "0  The paper presents an interesting approach to ...  \n",
       "1  The paper presents a novel investigation into ...  \n",
       "2  The paper presents a significant contribution ...  \n",
       "3  The paper addresses a novel and relevant probl...  \n",
       "4  The paper presents a novel approach to time se...  \n",
       "5  The paper presents the ADOPD-Instruct dataset,...  \n",
       "6  The paper proposes a low-rank autoregressive r...  \n",
       "7  The manuscript presents a novel approach, GVFi...  \n",
       "8  The paper introduces a novel solution concept,...  \n",
       "9  The manuscript presents a novel approach to Ga...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_results = validate_editor_on_human_reviews(n_papers=10)\n",
    "val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc978e9a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example paper_id: zkNCWtw2fd\n",
      "--- Simulating: 0 LLM reviewers (Round 1/1) ---\n",
      "Generating Editor decision...\n",
      "WARNING: OPENAI_API_KEY not set. Simulation calls will fail.\n",
      "[autogen.oai.client: 11-25 15:04:33] {356} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "LLM Editor failed: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "--- Simulating: 1 LLM reviewers (Round 1/1) ---\n",
      "Generating LLM review 1/1...\n",
      "WARNING: OPENAI_API_KEY not set. Simulation calls will fail.\n",
      "[autogen.oai.client: 11-25 15:04:33] {356} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "LLM Reviewer failed: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "Generating Editor decision...\n",
      "WARNING: OPENAI_API_KEY not set. Simulation calls will fail.\n",
      "[autogen.oai.client: 11-25 15:04:33] {356} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n",
      "LLM Editor failed: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\n",
      "Simulation Results:\n",
      "     paper_id  round  n_llm_reviewers  replace_human editor_decision  \\\n",
      "0  zkNCWtw2fd      1                0           True          reject   \n",
      "1  zkNCWtw2fd      1                1           True          reject   \n",
      "\n",
      "  editor_rationale editor_highlights  \n",
      "0             None              None  \n",
      "1             None              None  \n"
     ]
    }
   ],
   "source": [
    "# Example (skeleton): how decisions change as we increase the number of LLM reviews\n",
    "\n",
    "# Pick a paper_id that has at least one human review in the CSV\n",
    "if not reviews_df.empty:\n",
    "    example_paper_id = reviews_df[\"paper_id\"].iloc[0]\n",
    "    print(\"Example paper_id:\", example_paper_id)\n",
    "\n",
    "    # Uncomment to run (requires OPENAI_API_KEY env var)\n",
    "    comp_df = simulate_llm_composition_for_paper(\n",
    "        paper_id=example_paper_id,\n",
    "        llm_counts=[0, 1],        # Try 0 LLMs (all human) vs 1 LLM (switched)\n",
    "        replace_human=True,       # SWITCH mode\n",
    "        n_rounds=1,\n",
    "    )\n",
    "    print(\"\\nSimulation Results:\")\n",
    "    print(comp_df)\n",
    "else:\n",
    "    print(\"No reviews found to sample from.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-abm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
