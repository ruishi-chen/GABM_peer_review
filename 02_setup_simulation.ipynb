{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5335ad9f",
   "metadata": {},
   "source": [
    "### Generative agent-based peer review simulation\n",
    "\n",
    "This notebook sets up a simple **agent-based simulation of ICLR peer review decisions**.\n",
    "\n",
    "We use historical human reviews (ratings and decisions) to estimate paper-level quality, then simulate review panels where a varying proportion of reviewers are **LLM-based agents** instead of humans.\n",
    "\n",
    "By changing the fraction of agent reviewers, we can study how simulated **accept/reject decisions across manuscripts** shift as the reviewer population becomes more agent-heavy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e910644",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Path to the CSV created in 01_get_human_review.ipynb\n",
    "DATA_PATH = Path(\"ICLR2025_human_reviews.csv\")\n",
    "DATA_PATH.resolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess human review data\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_PATH.resolve()} â€” run 01_get_human_review.ipynb first to create it.\"\n",
    "    )\n",
    "\n",
    "reviews_df = pd.read_csv(DATA_PATH)\n",
    "print(\"Raw reviews shape:\", reviews_df.shape)\n",
    "reviews_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse numeric rating from strings like \"8: Strong accept\"\n",
    "\n",
    "def parse_rating(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    if isinstance(val, (int, float)):\n",
    "        return float(val)\n",
    "    s = str(val).strip()\n",
    "    # Try splitting on ':' first, then space\n",
    "    for sep in (\":\", \" \"):\n",
    "        try:\n",
    "            num = float(s.split(sep)[0])\n",
    "            return num\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "reviews_df[\"rating_num\"] = reviews_df[\"rating\"].apply(parse_rating)\n",
    "\n",
    "print(\"Fraction of reviews with parsed numeric rating:\", reviews_df[\"rating_num\"].notna().mean())\n",
    "reviews_df[[\"paper_id\", \"rating\", \"rating_num\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659dbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to paper-level statistics we will use in the simulation\n",
    "\n",
    "paper_stats = (\n",
    "    reviews_df.groupby(\"paper_id\")\n",
    "    .agg(\n",
    "        n_reviews=(\"review_id\", \"count\"),\n",
    "        mean_rating=(\"rating_num\", \"mean\"),\n",
    "        std_rating=(\"rating_num\", \"std\"),\n",
    "        decision=(\"decision\", lambda x: x.dropna().iloc[0] if len(x.dropna()) else np.nan),\n",
    "        title=(\"title\", \"first\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Paper-level stats shape:\", paper_stats.shape)\n",
    "paper_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map decisions to a simple accept / reject label and define a \"quality\" score\n",
    "\n",
    "def is_accept(decision: str) -> float:\n",
    "    if not isinstance(decision, str):\n",
    "        return np.nan\n",
    "    d = decision.lower()\n",
    "    # Very simple rule: any label containing \"accept\" and not containing \"reject\" counts as accept\n",
    "    if \"accept\" in d and \"reject\" not in d:\n",
    "        return 1.0\n",
    "    if \"reject\" in d:\n",
    "        return 0.0\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "paper_stats[\"accept_label\"] = paper_stats[\"decision\"].apply(is_accept)\n",
    "\n",
    "# Use the mean human rating as a proxy for latent paper quality\n",
    "# (you can swap this out for a more sophisticated quality estimate later)\n",
    "global_mean_rating = paper_stats[\"mean_rating\"].mean()\n",
    "paper_stats[\"quality\"] = paper_stats[\"mean_rating\"].fillna(global_mean_rating)\n",
    "\n",
    "paper_stats[[\"paper_id\", \"title\", \"quality\", \"accept_label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define human vs. agent reviewer behavior and panel decision rule\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "\n",
    "def sample_human_rating(quality: float, noise_std: float = 1.0, bias: float = 0.0) -> float:\n",
    "    \"\"\"Sample a synthetic human rating given latent paper quality.\n",
    "\n",
    "    Here quality and ratings live on the same numeric scale as the parsed ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    return quality + bias + rng.normal(0, noise_std)\n",
    "\n",
    "\n",
    "def sample_agent_rating(quality: float, noise_std: float = 0.8, bias: float = 0.0) -> float:\n",
    "    \"\"\"Sample a synthetic agent rating given latent paper quality.\n",
    "\n",
    "    As a default, we assume agents are *slightly less noisy* but you can tune bias/noise.\n",
    "    \"\"\"\n",
    "\n",
    "    return quality + bias + rng.normal(0, noise_std)\n",
    "\n",
    "\n",
    "def decide_from_ratings(ratings, threshold: float = 7.0) -> int:\n",
    "    \"\"\"Panel decision: accept if the mean rating crosses a threshold.\"\"\"\n",
    "\n",
    "    if len(ratings) == 0:\n",
    "        return 0\n",
    "    return int(np.mean(ratings) >= threshold)\n",
    "\n",
    "\n",
    "def simulate_paper_decisions(\n",
    "    quality: float,\n",
    "    n_reviewers: int = 3,\n",
    "    agent_share: float = 0.0,\n",
    "    n_trials: int = 100,\n",
    "    human_kwargs: dict | None = None,\n",
    "    agent_kwargs: dict | None = None,\n",
    "    threshold: float = 7.0,\n",
    ") -> float:\n",
    "    \"\"\"Return the *simulated acceptance probability* for one paper.\n",
    "\n",
    "    We repeatedly form panels of size `n_reviewers` with a given fraction of agent\n",
    "    reviewers, simulate ratings, and apply the decision rule.\n",
    "    \"\"\"\n",
    "\n",
    "    human_kwargs = human_kwargs or {}\n",
    "    agent_kwargs = agent_kwargs or {}\n",
    "\n",
    "    n_agents = int(round(n_reviewers * agent_share))\n",
    "    n_humans = n_reviewers - n_agents\n",
    "\n",
    "    outcomes = []\n",
    "    for _ in range(n_trials):\n",
    "        human_ratings = [sample_human_rating(quality, **human_kwargs) for _ in range(n_humans)]\n",
    "        agent_ratings = [sample_agent_rating(quality, **agent_kwargs) for _ in range(n_agents)]\n",
    "        ratings = human_ratings + agent_ratings\n",
    "        outcomes.append(decide_from_ratings(ratings, threshold=threshold))\n",
    "\n",
    "    return float(np.mean(outcomes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation for different proportions of agent reviewers\n",
    "\n",
    "agent_shares = np.linspace(0.0, 1.0, 5)  # e.g., 0%, 25%, 50%, 75%, 100%\n",
    "n_reviewers = 3\n",
    "n_trials = 200\n",
    "\n",
    "sim_rows: list[dict] = []\n",
    "\n",
    "for _, row in paper_stats.iterrows():\n",
    "    q = row[\"quality\"]\n",
    "    pid = row[\"paper_id\"]\n",
    "    for s in agent_shares:\n",
    "        accept_prob = simulate_paper_decisions(\n",
    "            q,\n",
    "            n_reviewers=n_reviewers,\n",
    "            agent_share=s,\n",
    "            n_trials=n_trials,\n",
    "            human_kwargs=dict(noise_std=1.0, bias=0.0),\n",
    "            agent_kwargs=dict(noise_std=0.8, bias=0.0),\n",
    "            threshold=7.0,\n",
    "        )\n",
    "        sim_rows.append(\n",
    "            {\n",
    "                \"paper_id\": pid,\n",
    "                \"agent_share\": s,\n",
    "                \"sim_accept_prob\": accept_prob,\n",
    "            }\n",
    "        )\n",
    "\n",
    "sim_results = pd.DataFrame(sim_rows)\n",
    "print(\"Simulation results shape:\", sim_results.shape)\n",
    "sim_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate over manuscripts: how does the average acceptance probability change?\n",
    "\n",
    "summary = (\n",
    "    sim_results.groupby(\"agent_share\")[\"sim_accept_prob\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sim_accept_prob\": \"mean_accept_prob\"})\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of agent share on overall simulated acceptance probability\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(data=summary, x=\"agent_share\", y=\"mean_accept_prob\", marker=\"o\")\n",
    "plt.xlabel(\"Fraction of reviewers who are agents\")\n",
    "plt.ylabel(\"Mean simulated acceptance probability\")\n",
    "plt.title(\"Effect of agent share on simulated ICLR acceptance rates\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much do individual manuscripts move when everyone becomes an agent reviewer?\n",
    "\n",
    "pivot = sim_results.pivot(index=\"paper_id\", columns=\"agent_share\", values=\"sim_accept_prob\")\n",
    "\n",
    "if 0.0 in pivot.columns and 1.0 in pivot.columns:\n",
    "    pivot[\"delta_all_agents_vs_all_humans\"] = pivot[1.0] - pivot[0.0]\n",
    "    pivot[\"delta_all_agents_vs_all_humans\"].describe()\n",
    "else:\n",
    "    print(\"Expected agent_share values 0.0 and 1.0 not found in simulation results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-abm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
